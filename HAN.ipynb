{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1099063, 30), (56370, 30), (59728, 300))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_data\n",
    "\n",
    "seq_arr, test_seq_arr, labels, word_index, index_word, vs, embedding_matrix = load_data('word', 'glove')\n",
    "seq_arr.shape, test_seq_arr.shape, embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0', '/device:GPU:1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import f1\n",
    "from keras import callbacks\n",
    "from timeit import default_timer as timer\n",
    "from keras import models, losses, metrics, layers, optimizers\n",
    "from keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "MAX_SEN_LEN = 10\n",
    "MAX_SEN = 3\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.4\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1099063, 30)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 10, 300)           17918400  \n",
      "_________________________________________________________________\n",
      "bidirectional_20 (Bidirectio (None, 10, 20)            18660     \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 20)                440       \n",
      "=================================================================\n",
      "Total params: 17,937,500\n",
      "Trainable params: 17,937,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SEN_LEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(10, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttentionWithContext()(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_input = Input(shape=(MAX_SEN, MAX_SEN_LEN), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(10, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttentionWithContext()(l_lstm_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        (None, 3, 10)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 3, 20)             17937500  \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 3, 20)             1860      \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 20)                440       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 17,939,821\n",
      "Trainable params: 17,939,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "preds = Dense(1, activation='sigmoid')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['acc', f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_22:0' shape=(?, 3, 10) dtype=int32>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network\n",
      "Train on 659437 samples, validate on 439626 samples\n",
      "Epoch 1/10\n",
      " 41750/659437 [>.............................] - ETA: 9:56 - loss: 0.1858 - acc: 0.9449 - f1: 0.1074"
     ]
    }
   ],
   "source": [
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(data, labels, validation_split = 0.4,\n",
    "          epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = seq_arr[100]\n",
    "\n",
    "max_sen = 3\n",
    "max_sen_len = 10\n",
    "\n",
    "data = np.zeros(shape = (1, max_sen, max_sen_len))\n",
    "\n",
    "clauses = []\n",
    "track = []\n",
    "j = 0\n",
    "\n",
    "for ii, idx in enumerate(s):\n",
    "    w = index_word[idx]\n",
    "    track.append(idx)\n",
    "    if w in punc:\n",
    "        j += 1\n",
    "        clauses.append(track)\n",
    "        track = []\n",
    "        \n",
    "data[0, 0:j, :] = pad_sequences(clauses, max_sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  16,  201,   71, 5545,    5, 5384,   61,   17,   29,    2],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 13, 2, 789, 1193, 202]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc = ['.', ',', '?', '!', ';', ':']\n",
    "punc_idx = [word_index[i] for i in punc]\n",
    "punc_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3, 10)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc = ['.', ',', '?', '!', ';', ':']\n",
    "punc_idx = [word_index[i] for i in punc]\n",
    "\n",
    "# Number of sequences to try\n",
    "trial_num = 1000\n",
    "\n",
    "# Maximum number of sentences\n",
    "max_sen = 3\n",
    "# Maximum words per sentence\n",
    "max_sen_len = 10\n",
    "\n",
    "# Data is initially all 0s\n",
    "data = np.zeros((trial_num, max_sen, max_sen_len))\n",
    "\n",
    "# Iterate through the sequences\n",
    "for i, s in enumerate(seq_arr[:trial_num]):    \n",
    "    \n",
    "    # Clauses is a list of lists\n",
    "    clauses = []\n",
    "    # Track is a single list\n",
    "    track = []\n",
    "    \n",
    "    # Number of clauses\n",
    "    j = 0\n",
    "\n",
    "    # Iterate through the sequence\n",
    "    for idx in s:\n",
    "        if j == max_sen:\n",
    "            break\n",
    "        \n",
    "        # Record the index\n",
    "        track.append(idx)\n",
    "        \n",
    "        # If we find punctuation\n",
    "        if idx in punc_idx:\n",
    "            j += 1\n",
    "            clauses.append(track)\n",
    "            # Reset the tracker\n",
    "            track = []\n",
    "\n",
    "    # Record the found clauses padded to the maximum length\n",
    "    data[i, 0:j, :] = pad_sequences(clauses, max_sen_len)\n",
    "    \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_clause_data(sequences,\n",
    "                max_sen, max_sen_len,\n",
    "                punc = ['.', ',', '?', '!', ';', ':']):\n",
    "    \"\"\"Break data into clauses\"\"\"\n",
    "\n",
    "    punc_idx = [word_index[i] for i in punc]\n",
    "\n",
    "    # Maximum number of sentences\n",
    "    max_sen = 3\n",
    "    # Maximum words per sentence\n",
    "    max_sen_len = 10\n",
    "\n",
    "    # Data is initially all 0s\n",
    "    data = np.zeros((len(sequences), max_sen, max_sen_len))\n",
    "\n",
    "    start = timer()\n",
    "    \n",
    "    # Iterate through the sequences\n",
    "    for i, s in enumerate(seq_arr):\n",
    "        # Track progress\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f'{100 * i / len(sequences):.2f}% complete.', end = '\\r')\n",
    "        \n",
    "        # Clauses is a list of lists\n",
    "        clauses = []\n",
    "        # Track is a single list\n",
    "        track = []\n",
    "\n",
    "        # Number of clauses\n",
    "        j = 0\n",
    "\n",
    "        # Iterate through the sequence\n",
    "        for idx in s:\n",
    "            # If we have already found enough sentences\n",
    "            if j == max_sen:\n",
    "                break\n",
    "\n",
    "            # Record the index\n",
    "            track.append(idx)\n",
    "\n",
    "            # If we find punctuation\n",
    "            if idx in punc_idx:\n",
    "                j += 1\n",
    "                clauses.append(track)\n",
    "                # Reset the tracker\n",
    "                track = []\n",
    "\n",
    "        # Record the found clauses padded to the maximum length\n",
    "        data[i, 0:j, :] = pad_sequences(clauses, max_sen_len)\n",
    "    \n",
    "    print(f'Formatted in {timer() - start:.2f} seconds.')\n",
    "    print('Final data shape: ', data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted in 317.92 seconds.\n",
      "Final data shape:  (1099063, 3, 10)\n"
     ]
    }
   ],
   "source": [
    "data = format_clause_data(seq_arr, max_sen = 3,\n",
    "                          max_sen_len = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_clause_data.npy', data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
