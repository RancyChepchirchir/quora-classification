{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1099063, 30), (56370, 30), (59728, 300))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only want to use one gpu\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # so the IDs match nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # \"0, 1\" for multiple\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())\n",
    "\n",
    "from utils import load_data\n",
    "\n",
    "seq_arr, test_seq_arr, labels, word_index, index_word, vs, embedding_matrix = load_data('word', 'glove')\n",
    "seq_arr.shape, test_seq_arr.shape, embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import f1\n",
    "from keras import callbacks\n",
    "from timeit import default_timer as timer\n",
    "from keras import models, losses, metrics, layers, optimizers\n",
    "from keras.callbacks import *\n",
    "from keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MAX_SEN_LEN = 15\n",
    "MAX_SEN = 5\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.73% complete.\r"
     ]
    }
   ],
   "source": [
    "def format_clause_data(sequences,\n",
    "                max_sen, max_sen_len,\n",
    "                punc = ['.', ',', '?', '!', ';', ':']):\n",
    "    \"\"\"Break data into clauses\"\"\"\n",
    "\n",
    "    punc_idx = [word_index[i] for i in punc]\n",
    "\n",
    "    # Data is initially all 0s\n",
    "    data = np.zeros((len(sequences), max_sen, max_sen_len))\n",
    "\n",
    "    start = timer()\n",
    "    \n",
    "    # Iterate through the sequences\n",
    "    for i, s in enumerate(seq_arr):\n",
    "        # Track progress\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f'{100 * i / len(sequences):.2f}% complete.', end = '\\r')\n",
    "        \n",
    "        # Clauses is a list of lists\n",
    "        clauses = []\n",
    "        # Track is a single list\n",
    "        track = []\n",
    "\n",
    "        # Number of clauses\n",
    "        j = 0\n",
    "\n",
    "        # Iterate through the sequence\n",
    "        for idx in s:\n",
    "            # If we have already found enough sentences\n",
    "            if j == max_sen:\n",
    "                break\n",
    "\n",
    "            # Record the index\n",
    "            track.append(idx)\n",
    "\n",
    "            # If we find punctuation\n",
    "            if idx in punc_idx:\n",
    "                j += 1\n",
    "                clauses.append(track)\n",
    "                # Reset the tracker\n",
    "                track = []\n",
    "\n",
    "        # Record the found clauses padded to the maximum length\n",
    "        data[i, 0:j, :] = pad_sequences(clauses, max_sen_len)\n",
    "    \n",
    "    print(f'Formatted in {timer() - start:.2f} seconds.')\n",
    "    print('Final data shape: ', data.shape)\n",
    "    return data\n",
    "\n",
    "data = format_clause_data(seq_arr, max_sen = MAX_SEN, max_sen_len = MAX_SEN_LEN)\n",
    "# data = np.load('word_clause_data.npy')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True,\n",
    "                            mask_zero=False)\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = False\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_input = Input(shape=(MAX_SEN_LEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(CuDNNGRU(5, return_sequences=True, \n",
    "                                kernel_regularizer=reg.l2()))(embedded_sequences)\n",
    "l_att = AttentionWithContext()(l_lstm)\n",
    "dense = Dense(32, activation = 'relu')(l_att)\n",
    "dense = Dropout(0.5)(dense)\n",
    "sentEncoder = Model(sentence_input, dense)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_input = Input(shape=(MAX_SEN, MAX_SEN_LEN), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(CuDNNGRU(5, return_sequences=True, \n",
    "                                     kernel_regularizer=reg.l2()))(review_encoder)\n",
    "l_att_sent = AttentionWithContext()(l_lstm_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = Dense(32, activation = 'relu')(l_att_sent)\n",
    "dense = Dropout(0.5)(dense)\n",
    "preds = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['binary_crossentropy',\n",
    "                       'acc', f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'word_han'\n",
    "\n",
    "callback_list = [EarlyStopping(monitor = 'val_loss', patience = 4),\n",
    "                 ModelCheckpoint(f'models/{model_name}.h5', monitor = 'val_loss',\n",
    "                             save_best_only = True)]\n",
    "\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(data, labels, validation_split = 0.4,\n",
    "          epochs=10, batch_size=1024, callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers as reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.l1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = seq_arr[100]\n",
    "\n",
    "max_sen = 3\n",
    "max_sen_len = 10\n",
    "\n",
    "data = np.zeros(shape = (1, max_sen, max_sen_len))\n",
    "\n",
    "clauses = []\n",
    "track = []\n",
    "j = 0\n",
    "\n",
    "for ii, idx in enumerate(s):\n",
    "    w = index_word[idx]\n",
    "    track.append(idx)\n",
    "    if w in punc:\n",
    "        j += 1\n",
    "        clauses.append(track)\n",
    "        track = []\n",
    "        \n",
    "data[0, 0:j, :] = pad_sequences(clauses, max_sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = ['.', ',', '?', '!', ';', ':']\n",
    "punc_idx = [word_index[i] for i in punc]\n",
    "punc_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = ['.', ',', '?', '!', ';', ':']\n",
    "punc_idx = [word_index[i] for i in punc]\n",
    "\n",
    "# Number of sequences to try\n",
    "trial_num = 1000\n",
    "\n",
    "# Maximum number of sentences\n",
    "max_sen = 3\n",
    "# Maximum words per sentence\n",
    "max_sen_len = 10\n",
    "\n",
    "# Data is initially all 0s\n",
    "data = np.zeros((trial_num, max_sen, max_sen_len))\n",
    "\n",
    "# Iterate through the sequences\n",
    "for i, s in enumerate(seq_arr[:trial_num]):    \n",
    "    \n",
    "    # Clauses is a list of lists\n",
    "    clauses = []\n",
    "    # Track is a single list\n",
    "    track = []\n",
    "    \n",
    "    # Number of clauses\n",
    "    j = 0\n",
    "\n",
    "    # Iterate through the sequence\n",
    "    for idx in s:\n",
    "        if j == max_sen:\n",
    "            break\n",
    "        \n",
    "        # Record the index\n",
    "        track.append(idx)\n",
    "        \n",
    "        # If we find punctuation\n",
    "        if idx in punc_idx:\n",
    "            j += 1\n",
    "            clauses.append(track)\n",
    "            # Reset the tracker\n",
    "            track = []\n",
    "\n",
    "    # Record the found clauses padded to the maximum length\n",
    "    data[i, 0:j, :] = pad_sequences(clauses, max_sen_len)\n",
    "    \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = format_clause_data(seq_arr, max_sen = 3,\n",
    "                          max_sen_len = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_clause_data.npy', data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
