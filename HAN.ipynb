{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Hierarchical Attention Network\n",
    "\n",
    "In this notebook, we develop a hierarchical attention network for text classification. This method is one of many competing for the most accurate method for natural language processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1099063, 30), (56370, 30), (59728, 300))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only want to use one gpu\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # so the IDs match nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # \"0, 1\" for multiple\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())\n",
    "\n",
    "from utils import load_data\n",
    "\n",
    "seq_arr, test_seq_arr, labels, word_index, index_word, vs, embedding_matrix = load_data('word', 'glove')\n",
    "seq_arr.shape, test_seq_arr.shape, embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import f1\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from keras.callbacks import *\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import regularizers as reg\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines the model parameters. `MAX_SEN` is the maximum number of sentences (clauses) for each question while `MAX_SEN_LEN` is the maximum number of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEN_LEN = 20\n",
    "MAX_SEN = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data\n",
    "\n",
    "Hierarchical attention networks require input of shape `[batch_size, sentences, words]`. We are dealing with questions, which often don't have more than one sentence. Therefore, we can split the questions into clauses based on punctuation. \n",
    "\n",
    "Each question will be broken into a maximum of 5 clauses, each of which has a maximum length of 20 words. The function below accomplishes this for all of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted in 304.08 seconds.\n",
      "Final data shape:  (1099063, 5, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1099063, 5, 20)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_clause_data(sequences,\n",
    "                max_sen, max_sen_len,\n",
    "                punc = ['.', ',', '?', '!', ';', ':']):\n",
    "    \"\"\"Break data into clauses\"\"\"\n",
    "\n",
    "    # Get indexes of punctuation\n",
    "    punc_idx = [word_index[i] for i in punc]\n",
    "\n",
    "    # Data is initially all 0s\n",
    "    data = np.zeros((len(sequences), max_sen, max_sen_len))\n",
    "\n",
    "    start = timer()\n",
    "    \n",
    "    # Iterate through the sequences\n",
    "    for i, s in enumerate(seq_arr):\n",
    "        # Track progress\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f'{100 * i / len(sequences):.2f}% complete.', end = '\\r')\n",
    "        \n",
    "        # Clauses is a list of lists\n",
    "        clauses = []\n",
    "        # Track is a single list\n",
    "        track = []\n",
    "\n",
    "        # Number of clauses\n",
    "        j = 0\n",
    "\n",
    "        # Iterate through the sequence\n",
    "        for idx in s:\n",
    "            # If we have already found enough sentences\n",
    "            if j == max_sen:\n",
    "                break\n",
    "\n",
    "            # Record the index\n",
    "            track.append(idx)\n",
    "\n",
    "            # If we find punctuation\n",
    "            if idx in punc_idx:\n",
    "                j += 1\n",
    "                clauses.append(track)\n",
    "                # Reset the tracker\n",
    "                track = []\n",
    "\n",
    "        # Record the found clauses padded to the maximum length\n",
    "        data[i, 0:j, :] = pad_sequences(clauses, max_sen_len)\n",
    "    \n",
    "    print(f'Formatted in {timer() - start:.2f} seconds.')\n",
    "    print('Final data shape: ', data.shape)\n",
    "    return data\n",
    "\n",
    "data = format_clause_data(seq_arr, max_sen = MAX_SEN, max_sen_len = MAX_SEN_LEN)\n",
    "# data = np.load('word_clause_data.npy')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'Do', 'you', 'have', 'an', 'adopted', 'dog', ',']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'how', 'would', 'you', 'encourage', 'people', 'to', 'adopt', 'and', 'not', 'shop', '?']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "example = data[1, :, :]\n",
    "\n",
    "for clause in example:\n",
    "    print([index_word[i] for i in clause])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'What', 'advice', 'do', 'you', 'have', 'for', 'anyone', 'who', 'wishes', 'to', 'accomplish', 'what', 'you', 'have', '?']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "example = data[100, :, :]\n",
    "\n",
    "for clause in example:\n",
    "    print([index_word[i] for i in clause])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model\n",
    "\n",
    "The next few cells define the model. We start off with an embedding layer for the words. We use pre-trained embeddings, but set the embeddings to be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True,\n",
    "                            mask_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer\n",
    "\n",
    "This is the main layer of the network. It uses attention with context to process sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = False\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 20, 300)           17918400  \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 20, 20)            24960     \n",
      "_________________________________________________________________\n",
      "attention_with_context_14 (A (None, 20)                440       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 32)                0         \n",
      "=================================================================\n",
      "Total params: 17,944,472\n",
      "Trainable params: 17,944,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Processes the sentences\n",
    "sentence_input = Input(shape=(MAX_SEN_LEN,), dtype='int32')\n",
    "# Embed sentences\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "# Apply a bi-directional lstm\n",
    "l_lstm = Bidirectional(CuDNNLSTM(10, return_sequences=True, \n",
    "                                 kernel_regularizer=reg.l2()))(embedded_sequences)\n",
    "# Apply the attention layer to the entire sequence\n",
    "l_att = AttentionWithContext()(l_lstm)\n",
    "# Apply a dense layer\n",
    "dense = Dense(32, activation = 'relu')(l_att)\n",
    "# Dropout\n",
    "dense = Dropout(0.5)(dense)\n",
    "\n",
    "# Create the model\n",
    "sentEncoder = Model(sentence_input, dense)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Model\n",
    "\n",
    "This model takes as input the clauses and applies the sentence encoded model to them. The sentence encoder is applied at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 5, 20)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 5, 32)             17944472  \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 5, 20)             3520      \n",
      "_________________________________________________________________\n",
      "attention_with_context_15 (A (None, 20)                440       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 17,949,137\n",
      "Trainable params: 17,949,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input is the clauses\n",
    "review_input = Input(shape=(MAX_SEN, MAX_SEN_LEN), dtype='int32')\n",
    "# Encode the clauses with the sentence encoded applied for each clause\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "\n",
    "# Apply a bidirectional lstm\n",
    "l_lstm_sent = Bidirectional(CuDNNLSTM(10, return_sequences=True, \n",
    "                                     kernel_regularizer=reg.l2()))(review_encoder)\n",
    "# Apply the attention layer with context\n",
    "l_att_sent = AttentionWithContext()(l_lstm_sent)\n",
    "\n",
    "# Apply a fully connected layer\n",
    "dense = Dense(32, activation = 'relu')(l_att_sent)\n",
    "# Apply dropout\n",
    "dense = Dropout(0.5)(dense)\n",
    "# Make predictions\n",
    "preds = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(review_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['binary_crossentropy',\n",
    "                       'acc', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_17:0' shape=(?, 5, 20) dtype=int32>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs to the model are `[batch_size, num_sentences, num_words_per_sentence]`. The main model applies the sentence model to each sentence in the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Train on 659437 samples, validate on 439626 samples\n",
      "Epoch 1/10\n",
      "659437/659437 [==============================] - 43s 66us/step - loss: 0.3939 - binary_crossentropy: 0.1954 - acc: 0.9431 - f1: 0.1456 - val_loss: 0.1278 - val_binary_crossentropy: 0.1212 - val_acc: 0.9514 - val_f1: 0.3884\n",
      "Epoch 2/10\n",
      "659437/659437 [==============================] - 40s 60us/step - loss: 0.1247 - binary_crossentropy: 0.1195 - acc: 0.9530 - f1: 0.4999 - val_loss: 0.1182 - val_binary_crossentropy: 0.1138 - val_acc: 0.9548 - val_f1: 0.5200\n",
      "Epoch 3/10\n",
      "659437/659437 [==============================] - 39s 59us/step - loss: 0.1132 - binary_crossentropy: 0.1087 - acc: 0.9570 - f1: 0.5744 - val_loss: 0.1252 - val_binary_crossentropy: 0.1210 - val_acc: 0.9549 - val_f1: 0.5492\n",
      "Epoch 4/10\n",
      "659437/659437 [==============================] - 39s 59us/step - loss: 0.1069 - binary_crossentropy: 0.1024 - acc: 0.9588 - f1: 0.6005 - val_loss: 0.1210 - val_binary_crossentropy: 0.1166 - val_acc: 0.9539 - val_f1: 0.5750\n",
      "Epoch 5/10\n",
      "659437/659437 [==============================] - 39s 59us/step - loss: 0.1023 - binary_crossentropy: 0.0979 - acc: 0.9604 - f1: 0.6256 - val_loss: 0.1250 - val_binary_crossentropy: 0.1208 - val_acc: 0.9538 - val_f1: 0.5735\n",
      "Epoch 6/10\n",
      "659437/659437 [==============================] - 39s 59us/step - loss: 0.0992 - binary_crossentropy: 0.0947 - acc: 0.9615 - f1: 0.6380 - val_loss: 0.1289 - val_binary_crossentropy: 0.1220 - val_acc: 0.9531 - val_f1: 0.5741\n"
     ]
    }
   ],
   "source": [
    "model_name = 'word_han'\n",
    "\n",
    "# Create callbacks\n",
    "callback_list = [EarlyStopping(monitor = 'val_loss', patience = 4),\n",
    "                 ModelCheckpoint(f'models/{model_name}.h5', monitor = 'val_loss',\n",
    "                                 save_best_only = True)]\n",
    "\n",
    "# Train the model\n",
    "print('Starting Training')\n",
    "history = model.fit(data, labels, validation_split = 0.4,\n",
    "          epochs=10, batch_size=1024, \n",
    "          callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_clause_data.npy', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            trainable=True,\n",
    "                            mask_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 5, 20)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 5, 32)             17944472  \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 5, 20)             3520      \n",
      "_________________________________________________________________\n",
      "attention_with_context_17 (A (None, 20)                440       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 17,949,137\n",
      "Trainable params: 17,949,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Processes the sentences\n",
    "sentence_input = Input(shape=(MAX_SEN_LEN,), dtype='int32')\n",
    "# Embed sentences\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "# Apply a bi-directional lstm\n",
    "l_lstm = Bidirectional(CuDNNLSTM(10, return_sequences=True, \n",
    "                                 kernel_regularizer=reg.l2()))(embedded_sequences)\n",
    "# Apply the attention layer to the entire sequence\n",
    "l_att = AttentionWithContext()(l_lstm)\n",
    "# Apply a dense layer\n",
    "dense = Dense(32, activation = 'relu')(l_att)\n",
    "# Dropout\n",
    "dense = Dropout(0.5)(dense)\n",
    "\n",
    "# Create the model\n",
    "sentEncoder = Model(sentence_input, dense)\n",
    "\n",
    "# Input is the clauses\n",
    "review_input = Input(shape=(MAX_SEN, MAX_SEN_LEN), dtype='int32')\n",
    "# Encode the clauses with the sentence encoded applied for each clause\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "\n",
    "# Apply a bidirectional lstm\n",
    "l_lstm_sent = Bidirectional(CuDNNLSTM(10, return_sequences=True, \n",
    "                                     kernel_regularizer=reg.l2()))(review_encoder)\n",
    "# Apply the attention layer with context\n",
    "l_att_sent = AttentionWithContext()(l_lstm_sent)\n",
    "\n",
    "# Apply a fully connected layer\n",
    "dense = Dense(32, activation = 'relu')(l_att_sent)\n",
    "# Apply dropout\n",
    "dense = Dropout(0.5)(dense)\n",
    "# Make predictions\n",
    "preds = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(review_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Train on 659437 samples, validate on 439626 samples\n",
      "Epoch 1/10\n",
      "659437/659437 [==============================] - 44s 67us/step - loss: 0.4015 - binary_crossentropy: 0.2134 - acc: 0.9442 - f1: 0.0969 - val_loss: 0.1296 - val_binary_crossentropy: 0.1258 - val_acc: 0.9504 - val_f1: 0.5196\n",
      "Epoch 2/10\n",
      "659437/659437 [==============================] - 40s 61us/step - loss: 0.1251 - binary_crossentropy: 0.1215 - acc: 0.9530 - f1: 0.4987 - val_loss: 0.1260 - val_binary_crossentropy: 0.1228 - val_acc: 0.9542 - val_f1: 0.5201\n",
      "Epoch 3/10\n",
      "659437/659437 [==============================] - 40s 60us/step - loss: 0.1111 - binary_crossentropy: 0.1082 - acc: 0.9577 - f1: 0.5805 - val_loss: 0.1179 - val_binary_crossentropy: 0.1155 - val_acc: 0.9552 - val_f1: 0.5657\n",
      "Epoch 4/10\n",
      "659437/659437 [==============================] - 40s 60us/step - loss: 0.1040 - binary_crossentropy: 0.1012 - acc: 0.9600 - f1: 0.6136 - val_loss: 0.1315 - val_binary_crossentropy: 0.1287 - val_acc: 0.9548 - val_f1: 0.5551\n",
      "Epoch 5/10\n",
      "659437/659437 [==============================] - 40s 61us/step - loss: 0.0997 - binary_crossentropy: 0.0969 - acc: 0.9614 - f1: 0.6325 - val_loss: 0.1290 - val_binary_crossentropy: 0.1263 - val_acc: 0.9538 - val_f1: 0.5410\n",
      "Epoch 6/10\n",
      "659437/659437 [==============================] - 40s 61us/step - loss: 0.0951 - binary_crossentropy: 0.0922 - acc: 0.9632 - f1: 0.6525 - val_loss: 0.1292 - val_binary_crossentropy: 0.1268 - val_acc: 0.9495 - val_f1: 0.5791\n",
      "Epoch 7/10\n",
      "659437/659437 [==============================] - 40s 61us/step - loss: 0.0924 - binary_crossentropy: 0.0896 - acc: 0.9645 - f1: 0.6679 - val_loss: 0.1294 - val_binary_crossentropy: 0.1267 - val_acc: 0.9524 - val_f1: 0.5441\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['binary_crossentropy',\n",
    "                       'acc', f1])\n",
    "\n",
    "model_name = 'word_han_no_pretrained'\n",
    "\n",
    "# Create callbacks\n",
    "callback_list = [EarlyStopping(monitor = 'val_loss', patience = 4),\n",
    "                 ModelCheckpoint(f'models/{model_name}.h5', monitor = 'val_loss',\n",
    "                                 save_best_only = True)]\n",
    "\n",
    "# Train the model\n",
    "print('Starting Training')\n",
    "history = model.fit(data, labels, validation_split = 0.4,\n",
    "          epochs=10, batch_size=1024, \n",
    "          callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
