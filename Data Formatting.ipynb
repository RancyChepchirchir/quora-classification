{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Formatting Data\n",
    "\n",
    "In this notebook, we format data for document classification. These documents are Quora questions that need to be labeled sincere or insincere. We will format the data in two ways, into words, or into characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paragram_300_sl999',\n",
       " 'GoogleNews-vectors-negative300',\n",
       " 'wiki-news-300d-1M',\n",
       " 'sample_submission.csv',\n",
       " 'train.csv',\n",
       " 'test.csv',\n",
       " 'glove.840B.300d',\n",
       " 'models']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import re\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "datadir = '/home/wjk68/data/quora/'\n",
    "\n",
    "os.listdir(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1306122, 56370)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(datadir + 'train.csv')\n",
    "texts = list(train['question_text'])\n",
    "labels = list(train['target'])\n",
    "\n",
    "test = pd.read_csv(datadir + 'test.csv')\n",
    "test_texts = list(test['question_text'])\n",
    "\n",
    "len(texts), len(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sequence(s):\n",
    "    \"\"\"Add spaces around punctuation.\"\"\"\n",
    "\n",
    "    # Add spaces around punctuation\n",
    "    s = re.sub(\n",
    "        r'(?<=[^\\s])(?=[“”!\\\"#$%&()*+,./:;<=>?@[\\]^_`{|}~\\t\\n])|(?=[^\\s])(?<=[“”!\\\"#$%&()*+,./:;<=>?@[\\]^_`{|}~\\t\\n])', r' ', s)\n",
    "\n",
    "    # Remove double spaces\n",
    "    s = re.sub(r'\\s\\s', ' ', s)\n",
    "    return s\n",
    "\n",
    "texts = [format_sequence(text) for text in texts]\n",
    "test_texts = [format_sequence(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 59727 words with more than 5 occurrences.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(lower = False, filters = '')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "wc = tokenizer.word_counts\n",
    "wc = sorted(wc.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "keep_min = 5\n",
    "keep = [count[0] for count in wc if count[1] >= keep_min]\n",
    "\n",
    "print(f'There are {len(keep)} words with more than {keep_min} occurrences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = len(keep), lower = False, \n",
    "                      filters = '', oov_token = 'UNK')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59728, 59727)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = dict(list(tokenizer.word_index.items())[:len(keep)])\n",
    "word_index['PAD'] = 0\n",
    "len(word_index), tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = dict(list(tokenizer.index_word.items())[:len(keep)])\n",
    "index_word[0] = 'PAD'\n",
    "vs = tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?', 1381190),\n",
       " ('the', 655017),\n",
       " ('What', 418491),\n",
       " ('to', 405796),\n",
       " ('a', 404386),\n",
       " ('in', 366895),\n",
       " ('is', 333568),\n",
       " ('of', 332834),\n",
       " ('I', 309502),\n",
       " ('How', 262904),\n",
       " ('and', 254031),\n",
       " (',', 235093),\n",
       " ('are', 213723),\n",
       " ('do', 212919),\n",
       " ('for', 201864)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = tokenizer.word_counts\n",
    "wc = sorted(wc.items(), key = lambda x: x[1], reverse = True)\n",
    "wc[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 503)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [len(s) for s in sequences]\n",
    "min(lens), max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(lens);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.306122e+06\n",
       "mean     1.443420e+01\n",
       "std      7.912828e+00\n",
       "min      1.000000e+00\n",
       "25%      9.000000e+00\n",
       "50%      1.200000e+01\n",
       "75%      1.700000e+01\n",
       "max      5.030000e+02\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(lens).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142121, 64938)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sequence_len = 8\n",
    "max_sequence_len = 30\n",
    "np.sum(np.array(lens) < min_sequence_len), np.sum(np.array(lens) > max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1099063"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [i for i, s in enumerate(sequences) if (len(s) >= min_sequence_len and len(s) <= max_sequence_len)]\n",
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1099063, 30), (56370, 30))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = pad_sequences(sequences, max_sequence_len)\n",
    "test_sequences = pad_sequences(test_sequences, max_sequence_len)\n",
    "\n",
    "seq_arr = np.array(sequences)[idx]\n",
    "labels = np.array(labels)[idx]\n",
    "test_seq_arr = np.array(test_sequences)\n",
    "\n",
    "seq_arr.shape, test_seq_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_sequences.npy', seq_arr)\n",
    "np.save('word_labels.npy', labels)\n",
    "np.save('test_word_sequences.npy', test_seq_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068893"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1188349"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4928745"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('word_word_index.json', 'w') as f:\n",
    "    f.write(json.dumps(word_index))\n",
    "    \n",
    "with open('word_index_word.json', 'w') as f:\n",
    "    f.write(json.dumps(index_word))\n",
    "    \n",
    "with open('word_wc.json', 'w') as f:\n",
    "    f.write(json.dumps(wc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_arr = np.load('word_sequences.npy')\n",
    "test_seq_arr = np.load('test_word_sequences.npy')\n",
    "labels = np.load('word_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = datadir + 'glove.840B.300d/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196017 /home/wjk68/data/quora/glove.840B.300d/glove.840B.300d.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l {EMBEDDING_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "glove_embeddings_index = []\n",
    "\n",
    "with open(EMBEDDING_FILE) as f:\n",
    "    for i, l in enumerate(f):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f'{100 * i / 2196017}% complete.', end = '\\r')\n",
    "        glove_embeddings_index.append(get_coefs(*l.split(\" \")))\n",
    "        \n",
    "glove_embeddings_index = dict(glove_embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = datadir + 'wiki-news-300d-1M/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l {EMBEDDING_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.899994949997476% complete.\r"
     ]
    }
   ],
   "source": [
    "wiki_embeddings_index = []\n",
    "\n",
    "with open(EMBEDDING_FILE) as f:\n",
    "    for i, l in enumerate(f):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f'{100 * i / 9999995}% complete.', end = '\\r')\n",
    "        wiki_embeddings_index.append(get_coefs(*l.split(\" \")))\n",
    "        \n",
    "wiki_embeddings_index = dict(wiki_embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings into matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index, embedding_index):\n",
    "    \n",
    "    embedding_matrix = np.random.normal(loc = 0, scale = 1/np.sqrt(vs), \n",
    "                                        size = (vs, embedding_index['the'].shape[0]))\n",
    "    not_found = 0\n",
    "    for i, word in enumerate(word_index.keys()):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f'{100 * i / len(word_index)}% complete.', end = '\\r')\n",
    "        \n",
    "        vector = embedding_index.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[i, :] = vector\n",
    "        else:\n",
    "            not_found += 1\n",
    "    \n",
    "    print(not_found, ' words were not found.')\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.740892043932494% complete.\r",
      "33.48345834449504% complete.\r",
      "50.22602464505759% complete.\r",
      "66.96859094562015% complete.\r",
      "83.7111572461827% complete.\r",
      "3132  words were not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(59728, 300)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings = create_embedding_matrix(word_index, glove_embeddings_index)\n",
    "glove_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3601  words were not found...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(59728, 300)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_embeddings = create_embedding_matrix(word_index, wiki_embeddings_index)\n",
    "wiki_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_glove_embeddings.npy', glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_wiki_embeddings.npy', wiki_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name, embedding_name):\n",
    "    if embedding_name == 'wiki':\n",
    "        if data_name == 'word':\n",
    "            embedding_matrix = np.load('word_wiki_embeddings.npy')\n",
    "    elif embedding_name == 'glove':\n",
    "        if data_name == 'word':\n",
    "            embedding_matrix = np.load('word_glove_embeddings.npy')\n",
    "            \n",
    "    if data_name == 'word':\n",
    "        seq_arr = np.load('word_sequences.npy')\n",
    "        test_seq_arr = np.load('test_word_sequences.npy')\n",
    "        labels = np.load('word_labels.npy')\n",
    "        iw = []\n",
    "        with open('word_index_word.json', 'r') as f:\n",
    "            for l in f:\n",
    "                iw.append(json.loads(l))\n",
    "\n",
    "        index_word = iw[0]\n",
    "        index_word = {int(key): word for key, word in index_word.items()}\n",
    "\n",
    "        wi = []\n",
    "        with open('word_word_index.json', 'r') as f:\n",
    "            for l in f:\n",
    "                wi.append(json.loads(l))\n",
    "\n",
    "        word_index = wi[0]\n",
    "        word_index = {word: int(index) for word, index in word_index.items()}\n",
    "            \n",
    "        vs = len(word_index)\n",
    "    return seq_arr, test_seq_arr, labels, word_index, index_word, vs, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1099063, 30), (56370, 30))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_arr, test_seq_arr, labels, word_index, index_word, vs, embedding_matrix = load_data('word', 'glove')\n",
    "seq_arr.shape, test_seq_arr.shape, embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Embeddings\n",
    "\n",
    "Next we'll look at embedding the data as characters. We can use the same Tokenizer method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 322 characters with more than 10 occurrences.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters = '', lower = False, char_level = True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "wc = tokenizer.word_counts\n",
    "wc = sorted(wc.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "keep_min = 10\n",
    "keep = [count[0] for count in wc if count[1] >= keep_min]\n",
    "\n",
    "print(f'There are {len(keep)} characters with more than {keep_min} occurrences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1306122, 56370)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = len(keep), lower = False,\n",
    "                      char_level = True,\n",
    "                      filters = '', oov_token = 'UNK')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "chars = tokenizer.texts_to_sequences(texts)\n",
    "test_chars = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "len(chars), len(test_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323, 322)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = dict(list(tokenizer.word_index.items())[:len(keep)])\n",
    "word_index['PAD'] = 0\n",
    "len(word_index), tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = dict(list(tokenizer.index_word.items())[:len(keep)])\n",
    "index_word[0] = 'PAD'\n",
    "vs = tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = tokenizer.word_counts\n",
    "wc = sorted(wc.items(), key = lambda x: x[1], reverse = True)\n",
    "wc[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(s) for s in chars]\n",
    "min(lens), max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_char_len = 30\n",
    "max_char_len = 150\n",
    "\n",
    "np.sum(np.array(lens) < min_char_len), np.sum(np.array(lens) > max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [i for i, c in enumerate(chars) if (len(c) >= min_char_len and len(c) <= max_char_len)]\n",
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = pad_sequences(chars, max_char_len)\n",
    "test_chars = pad_sequences(test_chars, max_char_len)\n",
    "\n",
    "chars = chars[idx]\n",
    "labels = np.array(labels)[idx]\n",
    "\n",
    "chars.shape, test_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('char_sequences.npy', chars)\n",
    "np.save('char_labels.npy', labels)\n",
    "np.save('test_char_sequences.npy', test_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('char_word_index.json', 'w') as f:\n",
    "    f.write(json.dumps(word_index))\n",
    "    \n",
    "with open('char_index_word.json', 'w') as f:\n",
    "    f.write(json.dumps(index_word))\n",
    "    \n",
    "with open('char_wc.json', 'w') as f:\n",
    "    f.write(json.dumps(wc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
